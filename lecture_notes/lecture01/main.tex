% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode
\input{structure.tex}

\title{Introduction to partial differential equations (PDEs)}
\subtitle{A gentle approach}
\date{16/2/2021}
\date{}
%\author{18.303 Linear Partial Differential Equations: Analysis and Numerics}
\institute{18.303 Linear Partial Differential Equations: Analysis and Numerics}
\titlegraphic{\hfill\includegraphics[height=2em]{MIT-logo.pdf}}

\begin{document}
	
	\maketitle
	
%	\begin{frame}{Table of contents}
%		\setbeamertemplate{section in toc}[sections numbered]
%		\tableofcontents%[hideallsubsections]
%	\end{frame}

%--------------------
% Practical information

\begin{frame}{Practical information}
	\begin{itemize}
		\item All the important information on class website \url{https://github.com/mitmath/18303/}
		\item The recorded lectures can be found from the class Canvas website
		\item We will use the Canvas site when needed (submitting assignments, announcements etc.)
		\item For the computational parts we will use \textsc{Julia} (Steve Johnson's tutorial on Friday at 5 pm)
		\item The class consists of the psets, the midterm, and a final project (I'll decide on the dates soon)
 	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%
\section{Why are PDEs important?}
%%%%%%%%%%%%%%

%--------------------
% Newton's law

\begin{frame}{Law of Motion}
%	\begin{columns}[T,onlytextwidth]
%		\column{0.45\textwidth}
		Newton's law of motion
		$$
		m \dd[2]{\vec{x}}{t} = \vec{f}
		$$
		\begin{itemize}
			\item Describes dynamics at low velocities ($ \ll $ speed of light)
			\item Is used to derive classical dynamics resulting in various differential equations
			\item Can be extended to relativistic velocities (Einstein)
		\end{itemize}
\end{frame}

%--------------------
% Elasticity

\begin{frame}{Hooke's law}
	\begin{columns}[T,onlytextwidth]
		\column{0.46\textwidth}
		Hooke's law
		$$
		\vec{f} = -k \vec{x}
		$$
		\begin{itemize}
			\item Elementary constituents of materials are in \emph{equilibrium} when their distance is set
			\item When this distance changes due to deformations, the system energy increases and stress sets in
			\item Hooke's law is used to derive a large family of different equations (dynamical or static) to describe elasticity
		\end{itemize}
		
		\column{0.08\linewidth}
		\column{0.46\textwidth}
		\vspace{4em}
		\begin{figure}
			\centering
			\includegraphics[width=0.9\linewidth]{bending_beam.png}
			\caption{A finite element calculation of a bending beam.}
		\end{figure}
	\end{columns}
\end{frame}

%--------------------
% Quantum mechanics

\begin{frame}{Quantum mechanics}
	\centering
	\begin{columns}[T,onlytextwidth]
		\column{0.5\textwidth}
		SchrÃ¶dinger equation
		$$
			i \hbar \pfrac{\psi}{t} = -\frac{\hbar^2}{2m} \Delta \psi \argone + V(\vec{x}) \psi
		$$
		\begin{itemize}
			\item PDEs describe time-evolution and static properties of quantum mechanics
			\item Different PDEs for relativistic phenomena e.g. spin degrees of freedom
			\item Also subatomic physics are written in terms of PDEs
		\end{itemize}
		
		\column{0.5\textwidth}
		\begin{figure}
			\includegraphics[width=0.8\linewidth]{BE_condensation.jpg}
			\caption{Bose-Einstein condensation of rubidium atoms. Image courtesy of NIST/JILA/CU-Boulder.}
		\end{figure}
	\end{columns}
\end{frame}


%--------------------
% General relativity

\begin{frame}{General relativity}
	\begin{columns}[T,onlytextwidth]
		\column{0.46\textwidth}
		Einstein's field equation
		$$
		G_{\mu \nu} + \Lambda g_{\mu \nu} = \frac{8 \pi G}{c^4} T_{\mu \nu}
		$$
		\begin{itemize}
			\item Describes gravity's relation to mass and spacetime 
			\item Used to derive precise trajectories for planets and galaxies
			\item Predicts e.g. black holes
			\item $ G_{\mu \nu} $ and $ g_{\mu \nu} $ are related to the geometry of the spacetime through differential equations (tensors are not covered in this class)
		\end{itemize}
		
		\column{0.08\textwidth}
		
		\column{0.46\textwidth}
		\vspace{2em}
		\begin{figure}
			\includegraphics[width=\linewidth]{merger}
			\caption{Simulation of a black hole merger event [SXS lensing].}
		\end{figure}
	\end{columns}
\end{frame}

%--------------------
% Black-scholes

\begin{frame}{Finance}
		Black-Scholes equation for the price of an option
		$$
		\pfrac{V}{t} = \frac{1}{2}\sigma^2 S^2 \pfrac[2]{V}{S} + r S \pfrac{V}{S} = rV
		$$
		\begin{itemize}
			\item Derived from assumed stochastic (random) dynamics of the stock market
			\item Describes the pricing of an option $ V $ as a function of time ($ t $) and the price of the underlying asset $ S $
			\item Is a sort of an \emph{diffusion equation} (important later on)
		\end{itemize}
\end{frame}

%--------------------
% Vector spaces

\begin{frame}{Vector spaces}
	\textbf{Vectors}
	\begin{itemize}
		\item During this class we use objects called \alert{vectors} (abstract vectors are denoted with a bold symbol e.g. $ \vec{f} $)
		\item Vectors are elements of a \alert{vector space} $ V $ defined over a \alert{field} $ F $
		\item This field could be e.g. the set of real numbers $ \R $ or complex numbers $ \CC $
	\end{itemize}
	\vspace{1em}
	\begin{itemize}
		\item The vectors are defined in a way that adding the vectors creates another vector
		\item Let $ \vec{f} $, $ \vec{g} \in V$ be vectors and $ \alpha $, $ \beta \in F$
		\item It follows that $ \alpha \vec{f} + \beta \vec{g} \in V $
	\end{itemize}
(See how this works for your usual vectors, say, in $ \R^3 $)
\end{frame}

%%%%%%%%%%%%%%%%%
\section{Vector spaces}
%%%%%%%%%%%%%%%%%

%--------------------
% Norms

\begin{frame}{Vector spaces}
	\textbf{\textsc{Norm}}
	\begin{itemize}
		\item We define an operation $ V \to \R_{+}$ called the \alert{norm} denoted by $ \norm{\cdot} $ ($ \R_+ $ is the set of nonnegative real numbers)
		\item The norm has the following properties for $ \vec{f} \in V $ and $ \alpha \in F $:
		\begin{enumerate}
			\item It is nonnegative i.e. $ \norm{\fone} \geq 0 $
			\item For nonzero vectors it is positive: $ \norm{\fone} = 0 \Leftrightarrow \fone = 0 $ 
			\item $ \norm{\alpha \fone} = |\alpha| \norm{\fone}$
			\item The triangle inequality holds (\textbf{important}): $ \norm{\fone + \ftwo} \leq \norm{\fone} + \norm{\ftwo} $
		\end{enumerate}
	\end{itemize}
%
{\color{olive}
Assume $ \norm{\fone_k} $ converges like a normal sequence of numbers (Cauchy). If there is $ \fone \in V $ s.t. $ \lim_{k \to \infty} \norm{\fone_k - \fone} = 0 $ for all such sequences, 
$ V $ is called a \emph{Banach space} and $ V $ is said to be \emph{complete} (not important for this class but our vectors will be in a Banach space). [I'll use this color to denote optional material]
}
\end{frame}

%--------------------
% Inner product

\begin{frame}{Vector spaces}
	The norm defines a \emph{metric} i.e. a notion of distance for vectors (if the norm of $ \fone - \ftwo $ is zero, they're the same vector). We will introduce a notion of angle between vectors by defining an \alert{inner product}:
	
	\textbf{\textsc{Inner product}}
	\begin{itemize}
		\item Inner product is a bilinear operation $ \langle \cdot \rangle : V \times V \to F $ with the following properties
		\begin{enumerate}
			\item Linearity: $ \langle \fthr, \alpha \fone + \beta \ftwo \rangle = \alpha \langle \fthr, \fone \rangle + \beta \langle \fthr, \ftwo \rangle $
			\item Conjugate symmetry: $ \langle \fone, \ftwo \rangle = \overline{\langle \ftwo, \fone \rangle} $ ($ \overline{\alpha} $ is the complex conjugate of $ \alpha $)
			\item Positive-definiteness: $ \langle \fone, \fone \rangle > 0 $ if $ \fone \neq 0 $
		\end{enumerate}
	\end{itemize}
For inner products $ \langle \fone, \fone \rangle = \norm{\fone}^2 $ i.e. it induces a norm on the vector space.

{\color{olive} Banach spaces with an inner product are called \emph{Hilbert spaces} (very important mathematical structure in quantum mechanics).}
\end{frame}

%--------------------
% Exercise

\begin{frame}{Exercise \theexercise}
%	\metroset{block=fill}
\centering
	\begin{block}{Inner product induces a norm}
		Show that $ \sqrt{\langle \fone, \fone \rangle} = \norm{\fone} $ is actually a norm
	\end{block}	
\end{frame}

\begin{frame}{Solution}
The first three properties of the norm follow pretty easily. Let's show the triangle inequality i.e.  $ \norm{\fone + \ftwo} \leq \norm{\fone} + \norm{\ftwo} $. We assume $ \fone $, $ \ftwo \neq 0$ (these cases work trivially).

Since both sides are positive, this is equivalent to {\color{purple} $ \norm{\fone + \ftwo}^2 \leq \norm{\fone}^2 + \norm{\ftwo}^2 + 2 \norm{\fone} \norm{\ftwo}$}.

We have $ \norm{\fone + \ftwo}^2 = \iprod{\fone+\ftwo}{\fone+\ftwo} = \iprod{\fone}{\fone}  + \iprod{\ftwo}{\ftwo} + \iprod{\fone}{\ftwo} + \iprod{\ftwo}{\fone}  =  \norm{\fone}^2 + \norm{\ftwo}^2 + 2 \Re \left( \iprod{\fone}{\ftwo} \right)$.

Now, it suffices to show that $ \Re \left( \iprod{\fone}{\ftwo} \right) \leq \norm{\fone} \norm{\ftwo} $ i.e. $ \Re \left( \iprod{\fone/\norm{\fone}}{\ftwo/\norm{\ftwo}} \right) 
=: {\color{magenta} \Re \left( \iprod{\hat{\fone}}{\hat{\ftwo}} \right)
\leq 1 }$.

Let $ \hat{\ftwo} = \hat{\fone} + \fthr $. 
Now we have $ \Re \left( \iprod{\hat{\fone}}{\hat{\ftwo}} \right) = \Re \left( \underbrace{\iprod{\hat{\fone}}{\hat{\fone}}}_{=1} \right) + \Re \left( \iprod{\hat{\fone}}{\fthr} \right)$. We see that it's enough to show that {\color{magenta}$ \Re \left( \iprod{\hat{\fone}}{\fthr} \right) \leq 0 $}.

We know that $ 1 = \iprod{\hat{\ftwo}}{\hat{\ftwo}} = \iprod{\hat{\fone} + \fthr}{\hat{\fone} + \fthr} = \underbrace{\norm{\hat{\fone}}^2}_{=1} + \norm{\fthr}^2 + 2\Re \left( \iprod{\hat{\fone}}{\fthr} \right)$. It follows that $ \Re \left( \iprod{\hat{\fone}}{\fthr} \right) = - \norm{\fthr}^2/2 \leq 0 $, which completes the proof.

\end{frame}

%--------------------
% Operators

\begin{frame}{Linear operators}	
	\textbf{\textsc{Linear operators}}
	\begin{itemize}
		\item Linear operator on a vector space $ \mathcal{L} (\cdot)  : V \to V $ maps vectors to vectors
		\item We write $ \mathcal{L}(\fone) = \mathcal{L} \fone $
		\item They are linear i.e. $ \mathcal{L}(\alpha \fone + \beta \ftwo) 
		= \alpha \mathcal{L} \fone + \beta \mathcal{L} \ftwo $
		\item Linearity of operators: for two operators $ \opone $ and $ \optwo $ we have $ \left( \alpha \opone + \beta \optwo \right) \fone = \alpha \opone \fone + \beta \optwo \ftwo $
	\end{itemize}
	\textbf{\textsc{Null space and range}}
	\begin{itemize}
		\item \alert{Null space} (kernel) $ N $ of an operator $ \opone $ is the set $ \{ \fone \in V : \opone \fone = 0 \} $
		\item \alert{Range} $ R $ of an operator $ \opone $ is the set $ \{ \fone \in V : \opone \ftwo = \fone\quad \text{for some $ \ftwo \in V $} \} $
	\end{itemize}
\end{frame}

%--------------------
% Exercise 

\begin{frame}{Exercise \theexercise}
	%	\metroset{block=fill}
	Let $ \fone $, $ \ftwo \in V $. What can you say about them if for a given operator $ \opone $ we have $ \opone \fone = \opone \ftwo $? What if the null space of $ \opone $ is $ \{ 0 \} $?
\end{frame}

%--------------------
% Heat equation

%	\begin{frame}{Heat equation}
%		\begin{equation*}
%			\pfrac{\psi \argone}{t} = \nabla \cdot \left[c \argone \nabla \psi \argone  \right]
%		\end{equation*}
%	\end{frame}

%--------------------
% Operators 2

\begin{frame}{Linear operators}	
	\textbf{\textsc{Eigenvalues and eigenvectors}}
	\begin{itemize}
		\item If $ \opone \fone = \lambda \fone $, we say that $ \fone $ is an \alert{eigenvector} of the operator $ \opone $ with an \alert{eigenvalue} $ \lambda $
	\end{itemize}
	\textbf{\textsc{Adjoints}}
	\begin{itemize}
		\item The adjoint of an operator $ \opone $, $ \opone^* $ is defined through the property $ \iprod{\fone}{\opone \ftwo} = \iprod{\opone^* \fone}{ \ftwo} $ for all $ \fone $, $ \ftwo \in V $
		\item If $ \opone = \opone^* $, the operator is called self-adjoint
		(think of symmetric (or Hermitian) matrices).
	\end{itemize}
\end{frame}

%--------------------
% Example: smooth functions

\begin{frame}{Example \theexample}	
	\textbf{\textsc{Smooth functions}}
	\begin{itemize}
		\item Let us define the vector space as a space of functions $ f: [0,1] \to \R $ such that arbitrarily high degrees of derivatives are continuous (we write $ f\in C^\infty $)
		\item We can define an inner product of $ f $ and $ g $ by $ \iprod{f}{g} = \int_{0}^{1} f(x)g(x) \diff x $
		\item If in addition we require that $ f(0)=f(1)=0 $, we write $ f \in C^\infty_0 $
		\item Functions in $ C^\infty_0 $ form an important category of functions called the \emph{test functions}
		\item Examples of linear operations for these vectors:
		\begin{itemize}
			\item The differentiation operator $ \dd[n]{}{x} $ for any integer $ n $
			\item The integral $ (\mathcal{I} f)(x) := \int_{0}^{x} f(x') \diff x' $
		\end{itemize}
	\end{itemize}
{\color{olive} In addition to the number of times functions can be differentiated, many times we also need to care if the integral $ \int_{0}^{1} |f|^p \diff x  $ is finite. If this is the case we write $ f \in L^p([0,1]) $. 
}
\end{frame}


%--------------------
% First differential equation 1

\begin{frame}{First differential equation}
	\metroset{block=fill}
	\begin{block}{\centering Poisson's equation with Dirichlet boundaries}
		 \begin{align*}
		 	&\pfrac[2]{u(x)}{x} = f(x), \\
		 	&u(0) = u(1) = 0.
		 \end{align*}
	\end{block}	
	
	\pause
	First, we solve the eigenvalue problem
	\begin{align*}
		&\pfrac[2]{\phi_n(x)}{x} = -\lambda_n^2 \phi(x), \\
		&u(0) = u(1) = 0.
	\end{align*}
	
	\pause
	The eigenfunctions (eigenvectors) of the differential operator $ \pp[2]{x} $ are exponential functions $e^{\pm i \lambda_n x} $. 
\end{frame}

%--------------------
% First differential equation 2

\begin{frame}{First differential equation}
	The eigenfunctions  have to respect the boundary conditions. We have $ \phi_n(0) = 0 $. How is this possible with the exponential functions?
	
	\pause
	Since we have two eigenfunctions ($e^{\pm i \lambda_n x} $), the linear combination is also an eigenfunction. Now $ \phi_n = \alpha e^{ i \lambda_n x} + \beta e^{ -i \lambda_n x} $ for some complex $ \alpha $ and $ \beta $. 
	
	\pause
	Solving for $ \phi_n(0) = 0 $ gives $ \beta = -\alpha $ and requiring that $ \phi_n $ is real gives $ \alpha = A/(2i) $ with some real $ A $. For now we can set $ A $ to 1 -- we just have to keep in mind that multiplying the eigenvector by a constant is also a solution. Now, \alert{$ \phi_n(x) = \sin(\lambda_n x) $}. 
	
	\pause
	The other boundary condition ($ \phi_n (1) = 0 $) gives $ \sin(\lambda_n) = 0 $. This is solved by \alert{$ \lambda_n = \pi n$} for any integer $ n $. Since $ \sin(-x) =-\sin(x) $ (a constant times $ \sin(x) $) and $ \sin(0) = 0 $, it suffices to have $ n = 1,2,3,... $
	
	\pause
	It turns out that $ \phi_n $ is a \alert{basis} for functions $ f:[0,1] \to \R $. {\color{olive} It was finally proven in 60's (Carleson (1966) \& Hunt (1968)) that any functions $ f $ (even ones that are not continuous) can be expressed in this basis \textit{iff} $ \int_{0}^{1} |f(x)|^p \diff x < \infty $ for some $ p>1 $. To be precise, the sine series converges almost everywhere to $ f $ with this condition (not at isolated points).}
	
\end{frame}

%--------------------
% Basis for vector spaces

\begin{frame}{Basis for vector spaces}	
	\textbf{\textsc{Basis}}
	\begin{itemize}
		\item A set of \alert{basis vectors} $ \{\ffor_n \}_{n=1}^\infty $ is a basis for the vector space $ V $ if the following properties hold:
		\begin{enumerate}
			\item \textbf{Linear independence}: $ \sum_{n=1}^{\infty} \alpha_n \ffor_{n} = 0 \Leftrightarrow \alpha_n = 0 $ for all $ n $
			\item \textbf{Spanning property}: any vector $ \fone \in V $ can be written as a linear combination of the basis vectors i.e. $ \fone = \sum_{n=1}^\infty \alpha_n \ffor_n $ for some $ \{ \alpha_n \} $ 
		\end{enumerate}
		\item The basis is said to be \alert{orthogonal} \textit{iff} $ \iprod{\ffor_i}{\ffor_j} = \beta_{i}
		\delta_{ij} $ ($ \delta_{ij} $ is called Kronecker delta; it's 1 if $ i = j $ and 0 otherwise) 
		\item If constants $ \beta_{i} = 1 $ for all $ i$, the basis is called \alert{orthonormal}
	\end{itemize}
\end{frame}


%--------------------
% Exercise

\begin{frame}{Exercise}
	%	\metroset{block=fill}
%	\centering
	Show that the basis $ \phi_n (x) = \sin(\pi n x) $ is orthogonal on the interval $x \in [0,1] $. 
	
	Is it orthonormal? 
	
	If not, how could you make it orthonormal?
\end{frame}

%--------------------
% First differential equation 3

\begin{frame}{First differential equation}
	\metroset{block=fill}
	\begin{block}{\centering Poisson's equation with Dirichlet boundaries}
		\begin{align*}
			&\pfrac[2]{u(x)}{x} = f(x), \\
			&u(0) = u(1) = 0.
		\end{align*}
	\end{block}
\end{frame}

%--------------------
% First differential equation 4

\begin{frame}{First differential equation}
	
	We write both $ u $ and $ f $ in the \alert{eigenbasis} $ \phi_n $. 
	We have $ u(x) = \sum_{n=1}^\infty \hat{u}_n \phi_n $ and $ f(x) = \sum_{n=1}^\infty \hat{f}_n \phi_n $. 
	
	\pause
	Inserting this in the differential equation gives 
	\[ \pfrac[2]{}{x} \sum_{n=1}^\infty \hat{u}_n \phi_n 
	= \sum_{n=1}^\infty \hat{u}_n \pfrac[2]{}{x} \phi_n
	= -\sum_{n=1}^\infty \hat{u}_n \lambda_n^2 \phi_n
	= \sum_{n=1}^\infty \hat{f}_n \phi_n.
	\]
	
	\pause
	We can take the inner product of both sides of the equation with $ \iprod{\phi_k}{\cdot} $. Since $ \phi_n $ are orthogonal we get $ - \hat{u}_{k} \lambda_k^2 = \hat{f}_k $ giving
	\[ \hat{u}_k = -\frac{\hat{f}_k}{\lambda_k^2} = -\frac{\hat{f}_k}{\pi^2 k^2}, \]
	where $ k=1,2,3,... $
	
%	How do we calculate the coefficients $ \hat{u}_n $ and $ \hat{f}_n $?
	
\end{frame}

%--------------------
% First differential equation 5

\begin{frame}{First differential equation}
	
	How do we calculate the coefficients  $ \hat{f}_n $?
	
	\pause
	We found out earlier that $ \iprod{\phi_i}{\phi_j} = \frac{1}{2} \delta_{ij} $ and we have 
	\[ \sum_{n=1}^\infty \hat{f}_n \phi_n = f(x). \]
	Taking the product $ \iprod{\phi_k}{\cdot} $ gives 
	\[ \frac{1}{2} \hat{f}_k = \int_{0}^{1} \phi_k(x) f(x) \diff x = \int_{0}^{1} \sin(k\pi x) f(x) \diff x  \]
	
	\pause
	Solving this gives a general formula for the sine series coefficients
	\begin{equation}\label{eq:sine_series_coeff}
		\alert{\hat{f}_k = 2 \int_{0}^{1} \sin(k\pi x) f(x) \diff x.}
	\end{equation}
\end{frame}

%--------------------
% Fourier transform

\begin{frame}{Fourier transform}
	The operation for calculating the sine series coefficients can be seen as a \alert{Fourier transform}. We write $ \hat{f} = \fouriert(f) $, where 
	\begin{equation}\label{eq:fourier_transform}
		(\fouriert f)_k := 2 \int_{0}^{1} \sin(k\pi x) f(x) \diff x.
	\end{equation}

	Now $ \fouriert: V \to W $, where $ W $ is the vector space of coefficients $ \hat{f}_n $ is a linear map (check for yourself). It also has an inverse $ \fouriert^{-1} $ defined through
	\[ \fouriert^{-1}(\hat{f})(x) = \sum_{n=1}^{\infty} \hat{f}_n \sin(n\pi x) \]
	i.e. $ \fouriert $ is a bijection between spaces $ V $ and $ W $ (for mathematical nitpicking we require that $ V $ are the vectors for which Fourier transform exists).
	
\end{frame}

%--------------------
% Notation

%\begin{frame}{Notation}
%	\begin{table}
%		\caption{Some notation for differential operators}
%		\begin{tabular}{lccr}
%			\toprule
%			Name & Notation & Alternative notation & Cartesian coordinates \\
%			\midrule
%			Total differential & $ \dd{f}{x} $ & - & - \\
%			Partial differential & $ \pfrac{f}{x} $ & $ \pp{x} f $ & -\\
%			Gradient & $ \nabla \cdot f $ & $ \operatorname{grad}(f) $ & $ (\pp{x}f,\pp{y}f) $ \\
%			Divergence & $ \nabla \cdot \fone $ & $ \operatorname{div}(\fone) $ & $ \pp{x} f_x + \pp{y} f_y $ \\
%			Curl & $ \nabla \times \fone $ & $ \nabla \wedge \fone $ & $ \pp{x}f_y - \pp{y} f_x $ \\
%			Laplacian & $ \Delta f$ & $ \nabla^2 f $ &  $ (\pp[2]{x} + \pp[2]{y})f $ \\
%			\bottomrule		
%		\end{tabular}
%	\end{table}
%\end{frame}

\end{document}
